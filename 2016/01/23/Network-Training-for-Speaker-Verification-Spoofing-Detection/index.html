<!doctype html>
<html class="theme-next   use-motion ">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.4.5.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Nanxin Chen, Computer Science, Machine Learning, Artificial Intelligence, Speech, Natural Language Processing, NLP, Kaggle, Neural Network, LSTM" />





  <link rel="alternate" href="/atom.xml" title="Share and Enjoy" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />






<meta name="description" content="I firstly noticed this question when I worked as an intern in AISpeech.">
<meta property="og:type" content="article">
<meta property="og:title" content="Network Training for Speaker Verification/ Spoofing Detection">
<meta property="og:url" content="http://www.myemacs.com/2016/01/23/Network-Training-for-Speaker-Verification-Spoofing-Detection/index.html">
<meta property="og:site_name" content="Share and Enjoy">
<meta property="og:description" content="I firstly noticed this question when I worked as an intern in AISpeech.">
<meta property="og:updated_time" content="2016-01-30T15:27:13.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Network Training for Speaker Verification/ Spoofing Detection">
<meta name="twitter:description" content="I firstly noticed this question when I worked as an intern in AISpeech.">



<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post',
    motion: true
  };
</script>

  <title> Network Training for Speaker Verification/ Spoofing Detection | Share and Enjoy </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'bobchennan', 'auto');
  ga('send', 'pageview');
</script>





  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Share and Enjoy</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">CNX's blog</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-publications">
          <a href="/publications" rel="section">
            
              <i class="menu-item-icon fa fa-heartbeat fa-fw"></i> <br />
            
            Publications
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            Tags
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content">
          

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Network Training for Speaker Verification/ Spoofing Detection
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            Posted on
            <time itemprop="dateCreated" datetime="2016-01-23T15:19:55+00:00" content="Jan 23 2016">
              Jan 23 2016
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/01/23/Network-Training-for-Speaker-Verification-Spoofing-Detection/#comments" itemprop="discussionUrl">
                <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/01/23/Network-Training-for-Speaker-Verification-Spoofing-Detection/" itemprop="commentsCount"></span>
              </a>
            </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><p>I firstly noticed this question when I worked as an intern in AISpeech.<br><a id="more"></a><br>When you have thousands of speakers in your training data, it is much harder to train a neural network predicting the right label. The differences between syllables are too obvious such that it is very hard to classify speakers with different texts. </p>
<p>In speaker verification tasks this problem is much easier to be solved. In my experience the ASR network which predicts tri-phone labels works pretty well for the text-dependent speaker verification (I don’t have such experience on text-independent one). In Interspeech 2015 I have shown the result given by LSTM network, which works amazingly well for the short-duration case. </p>
<p>For the spoofing detection the problem becomes much complex since it is much hard to get the word-level labels like senones. Recently I’m preparing for the challenge released by Idiap and working on new network structures for spoofing. But unfortunatelly the basic DNN network got very bad performance on the ASVspoof 2015 dataset. Until now I still don’t know the exact reason but there must be some differences between TNet and my Lasagne implementation, such as randomization process.</p>
<p>Generally this is a sequence label problem and models such as BLSTM should work well in my perspective. I will working on this problem for next few weeks.</p>
<p>BTW. I have read <a href="www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994">slides</a> given by  Christopher Moody. The idea of lda2vec is very interesting and it reminds me the co-reference project. Balance between local and global information is always a trick to get succeed for NLP tasks.</p>
<p>Code(inspired by the work for CIFAR-10):</p>
<figure class="highlight python"><figcaption><span>spoof</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span><br><span class="line">Lasagne implementation of CIFAR-10 examples from "Deep Residual Learning for Image Recognition" (http://arxiv.org/abs/1512.03385)</span><br><span class="line"></span><br><span class="line">With n=5, i.e. 32-layer network from the paper, this achieves a validation error of 6.88% (vs 7.51% in the paper).</span><br><span class="line">The accuracy has not yet been tested for the other values of n.</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">import</span> theano.tensor <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">import</span> lasagne</span><br><span class="line"><span class="keyword">from</span> htk <span class="keyword">import</span> HTKFeat_read</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="comment">### config ###</span></span><br><span class="line"></span><br><span class="line">FEATURE_DIM = <span class="number">48</span></span><br><span class="line">FEATURE_EX  = <span class="number">15</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">1024</span></span><br><span class="line">OUTPUT_SIZE = <span class="number">6</span></span><br><span class="line"><span class="comment">#FILES_SHOW = 5000</span></span><br><span class="line">data = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(fea_str, prefix)</span>:</span></span><br><span class="line">    s = fea_str.split(<span class="string">'='</span>)</span><br><span class="line">    t = s[<span class="number">1</span>].split(<span class="string">'['</span>)</span><br><span class="line">    u = t[<span class="number">1</span>].split(<span class="string">','</span>)</span><br><span class="line">    v = u[<span class="number">1</span>].split(<span class="string">']'</span>)</span><br><span class="line">    <span class="keyword">return</span> prefix+<span class="string">"_"</span>+s[<span class="number">0</span>], t[<span class="number">0</span>], int(u[<span class="number">0</span>]), int(v[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(scp, mlf, prefix=<span class="string">"train"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> data</span><br><span class="line">    tr = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> open(mlf,<span class="string">"r"</span>):</span><br><span class="line">        j = i.rstrip().split(<span class="string">'\t'</span>)</span><br><span class="line">        tr[prefix+<span class="string">"_"</span>+j[<span class="number">0</span>]] = int(j[<span class="number">1</span>])</span><br><span class="line">    file_data = &#123;&#125;</span><br><span class="line">    X = []</span><br><span class="line">    Y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> open(scp,<span class="string">"r"</span>):</span><br><span class="line">        name, file, start, end = parse(i, prefix)</span><br><span class="line">        <span class="comment">#if not name in X.keys():</span></span><br><span class="line">        <span class="comment">#    X[name]=[]</span></span><br><span class="line">        <span class="comment">#    Y[name]=[]</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> file <span class="keyword">in</span> file_data.keys():</span><br><span class="line">            file_data[file]=HTKFeat_read(file).getall()</span><br><span class="line">        <span class="keyword">if</span> data <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            curr = <span class="number">0</span></span><br><span class="line">            data = file_data[file][start-FEATURE_EX:end+FEATURE_EX+<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            curr = data.shape[<span class="number">0</span>]</span><br><span class="line">            data = np.vstack((data, file_data[file][start-FEATURE_EX:end+FEATURE_EX+<span class="number">1</span>]))</span><br><span class="line">        <span class="comment">#print(str(curr)+":"+str(start)+","+str(end))</span></span><br><span class="line">        X.extend(range(curr, curr+end-start))</span><br><span class="line">        <span class="comment">#print(str(X))</span></span><br><span class="line">        Y.extend([tr[name] <span class="keyword">for</span> i <span class="keyword">in</span> range(end-start)])</span><br><span class="line">    <span class="comment">#del file_data</span></span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># ##################### Build the neural network model #######################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#from lasagne.layers import Conv2DLayer as ConvLayer</span></span><br><span class="line"><span class="keyword">from</span> lasagne.layers <span class="keyword">import</span> ElemwiseSumLayer</span><br><span class="line"><span class="keyword">from</span> lasagne.layers <span class="keyword">import</span> InputLayer</span><br><span class="line"><span class="keyword">from</span> lasagne.layers <span class="keyword">import</span> DenseLayer</span><br><span class="line"><span class="keyword">from</span> lasagne.layers <span class="keyword">import</span> PadLayer</span><br><span class="line"><span class="keyword">from</span> lasagne.layers <span class="keyword">import</span> NonlinearityLayer</span><br><span class="line"><span class="keyword">from</span> lasagne.nonlinearities <span class="keyword">import</span> softmax, rectify</span><br><span class="line"></span><br><span class="line"><span class="comment"># NB! from pull request #461 : https://github.com/f0k/Lasagne/blob/98b5581fa830cda3d3f838506ef14e5811a35ef7/lasagne/layers/normalization.py</span></span><br><span class="line"><span class="keyword">from</span> lasagne.layers <span class="keyword">import</span> batch_norm</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dnn</span><span class="params">(input_var=None, n=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">residual_block</span><span class="params">(l, increase_dim=<span class="number">0</span>, projection=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> increase_dim&gt;<span class="number">0</span>:</span><br><span class="line">            stack_1 = batch_norm(NonlinearityLayer(DenseLayer(l, num_units=increase_dim, W=lasagne.init.HeNormal(gain=<span class="string">'relu'</span>)),nonlinearity=rectify))</span><br><span class="line">            stack_2 = batch_norm(NonlinearityLayer(DenseLayer(stack_1, num_units=increase_dim, W=lasagne.init.HeNormal(gain=<span class="string">'relu'</span>)),nonlinearity=rectify))</span><br><span class="line">            <span class="keyword">if</span> projection:</span><br><span class="line">                projection = DenseLayer(l, num_units=increase_dim, W=lasagne.init.HeNormal(gain=<span class="string">'relu'</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                projection = PadLayer(l, [tuple([<span class="number">0</span>, increase_dim-l.output_shape[<span class="number">1</span>]])], batch_ndim=<span class="number">1</span>)</span><br><span class="line">            block = NonlinearityLayer(batch_norm(ElemwiseSumLayer([projection, stack_2])),nonlinearity=rectify)</span><br><span class="line">			        <span class="keyword">else</span>:</span><br><span class="line">            stack_1 = batch_norm(NonlinearityLayer(DenseLayer(l, num_units=l.output_shape[<span class="number">1</span>], W=lasagne.init.HeNormal(gain=<span class="string">'relu'</span>)),nonlinearity=rectify))</span><br><span class="line">            stack_2 = batch_norm(NonlinearityLayer(DenseLayer(stack_1, num_units=l.output_shape[<span class="number">1</span>], W=lasagne.init.HeNormal(gain=<span class="string">'relu'</span>)),nonlinearity=rectif</span><br><span class="line">            block = NonlinearityLayer(batch_norm(ElemwiseSumLayer([l, stack_2])),nonlinearity=rectify)</span><br><span class="line">        <span class="keyword">return</span> block</span><br><span class="line"></span><br><span class="line">    l_in=InputLayer(shape=(<span class="keyword">None</span>, FEATURE_DIM*(<span class="number">2</span>*FEATURE_EX+<span class="number">1</span>)), input_var=input_var)</span><br><span class="line">    l=residual_block(l_in, HIDDEN_SIZE, <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> xrange(n):</span><br><span class="line">        l=residual_block(l)</span><br><span class="line">    network = DenseLayer(l, num_units=OUTPUT_SIZE, nonlinearity=softmax)</span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_mlp</span><span class="params">(input_var=None, n=<span class="number">4</span>)</span>:</span></span><br><span class="line">    l=InputLayer(shape=(<span class="keyword">None</span>, FEATURE_DIM*(<span class="number">2</span>*FEATURE_EX+<span class="number">1</span>)), input_var=input_var)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> xrange(n):</span><br><span class="line">        l=NonlinearityLayer(DenseLayer(l, num_units=HIDDEN_SIZE, W=lasagne.init.HeNormal(gain=<span class="string">'relu'</span>)),nonlinearity=rectify)</span><br><span class="line">    network = DenseLayer(l, num_units=OUTPUT_SIZE, nonlinearity=softmax)</span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"><span class="comment"># ############################# Batch iterator ###############################</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterate_minibatches</span><span class="params">(inputs, targets, batchsize, shuffle=False, augment=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        inds=range(len(inputs))</span><br><span class="line">        np.random.shuffle(inds)</span><br><span class="line">        inputs=[inputs[j] <span class="keyword">for</span> j <span class="keyword">in</span> inds]</span><br><span class="line">        targets=[targets[j] <span class="keyword">for</span> j <span class="keyword">in</span> inds]</span><br><span class="line">    <span class="comment">#nt=FILES_NUM</span></span><br><span class="line">    <span class="comment">#files=[HTKFeat_read(filename=file_pos[names[i]]) for i in xrange(FILES_NUM)]</span></span><br><span class="line">    <span class="comment">#file_names=[names[i] for i in xrange(FILES_NUM)]</span></span><br><span class="line">    <span class="comment">#inds=[0 for i in xrange(FILES_NUM)]</span></span><br><span class="line">    <span class="comment">#inds=[0 for i in xrange(len(names))]</span></span><br><span class="line">    <span class="comment">#avail=Set([i for i in xrange(len(inputs))])</span></span><br><span class="line">    curr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        X=<span class="keyword">None</span></span><br><span class="line">        Y=[]</span><br><span class="line">        finished=<span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> xrange(batchsize):</span><br><span class="line">            <span class="keyword">if</span> len(inputs)==curr:</span><br><span class="line">                finished=<span class="keyword">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            idx=curr</span><br><span class="line">            curr+=<span class="number">1</span></span><br><span class="line">            <span class="comment">#files[idx].seek(inputs[file_names[idx]][inds[idx]])</span></span><br><span class="line">            <span class="comment">#XX=[]</span></span><br><span class="line">            <span class="comment">#for _ in xrange(FEATURE_EX):</span></span><br><span class="line">            <span class="comment">#    XX.extend(files[idx].next().tolist())</span></span><br><span class="line">            XX=data[inputs[idx]:inputs[idx]+<span class="number">2</span>*FEATURE_EX+<span class="number">1</span>].ravel()</span><br><span class="line">            <span class="comment">#XX=data[inputs[names[idx]][inds[idx]]:inputs[names[idx]][inds[idx]]+2*FEATURE_EX+1].ravel()</span></span><br><span class="line">            <span class="comment">#if np.isnan(np.sum(XX)):</span></span><br><span class="line">            <span class="comment">#    print(names[idx]+":"+str(inputs[names[idx]][inds[idx]])+","+str(inputs[names[idx]][inds[idx]]+FEATURE_EX))</span></span><br><span class="line">            <span class="keyword">if</span> X <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                X=XX</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                X=np.vstack((X,XX))</span><br><span class="line">            Y.append(targets[idx])</span><br><span class="line">            <span class="comment">#Y.append(targets[names[idx]][inds[idx]])</span></span><br><span class="line">            <span class="comment">#inds[idx]+=1</span></span><br><span class="line">            <span class="comment">#if inds[idx]&gt;=len(inputs[names[idx]]):</span></span><br><span class="line">            <span class="comment">#   avail.remove(idx)</span></span><br><span class="line">               <span class="comment">#files[idx]=None</span></span><br><span class="line">               <span class="comment">#file_names[idx]=None</span></span><br><span class="line">               <span class="comment">#inds[idx]=0</span></span><br><span class="line">        <span class="comment">#print(str(X.mean(axis=1)))</span></span><br><span class="line">        <span class="keyword">yield</span> lasagne.utils.floatX(X),np.array(Y).astype(<span class="string">'int32'</span>)</span><br><span class="line">        <span class="keyword">if</span> finished:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ############################## Main program ################################</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(n=<span class="number">5</span>, num_epochs=<span class="number">50</span>)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> data</span><br><span class="line">    <span class="comment"># Load the dataset</span></span><br><span class="line">    print(<span class="string">"Loading data..."</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">"spoof.pickle"</span>, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            tmp = cPickle.load(f)</span><br><span class="line">            <span class="keyword">if</span> tmp!=FEATURE_EX:</span><br><span class="line">                print(<span class="string">"Context window don't match."</span>)</span><br><span class="line">                <span class="keyword">raise</span> ValueError,<span class="string">'invalid window'</span></span><br><span class="line">            data, X_train, Y_train, X_test, Y_test = cPickle.load(f)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Regenerate data!"</span>)</span><br><span class="line">        X_train, Y_train = load_data(<span class="string">"spoof_train.scp"</span>, <span class="string">"mlf"</span>)</span><br><span class="line">        X_test, Y_test = load_data(<span class="string">"spoof_dev.scp"</span>, <span class="string">"mlf"</span>,<span class="string">"dev"</span>)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">"spoof.pickle"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            cPickle.dump(FEATURE_EX, f)</span><br><span class="line">            cPickle.dump([data, X_train, Y_train, X_test, Y_test], f)</span><br><span class="line">    <span class="comment">#data = StandardScaler().fit_transform(data)</span></span><br><span class="line">    <span class="comment"># Prepare Theano variables for inputs and targets</span></span><br><span class="line">    input_var = T.matrix(<span class="string">'inputs'</span>)</span><br><span class="line">    target_var = T.ivector(<span class="string">'targets'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create neural network model</span></span><br><span class="line">    print(<span class="string">"Building model and compiling functions..."</span>)</span><br><span class="line">    network = build_mlp(input_var, n)</span><br><span class="line">    print(<span class="string">"number of parameters in model: %d"</span> % lasagne.layers.count_params(network))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a loss expression for training, i.e., a scalar objective we want</span></span><br><span class="line">    <span class="comment"># to minimize (for our multi-class problem, it is the cross-entropy loss):</span></span><br><span class="line">    prediction = lasagne.layers.get_output(network)</span><br><span class="line">    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)</span><br><span class="line">    loss = loss.mean()</span><br><span class="line">    <span class="comment"># add weight decay</span></span><br><span class="line">    all_layers = lasagne.layers.get_all_layers(network)</span><br><span class="line">    l2_penalty = lasagne.regularization.regularize_layer_params(all_layers, lasagne.regularization.l2) * <span class="number">0.0001</span></span><br><span class="line">    loss = loss + l2_penalty</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create update expressions for training</span></span><br><span class="line">    <span class="comment"># Stochastic Gradient Descent (SGD) with momentum</span></span><br><span class="line">    params = lasagne.layers.get_all_params(network, trainable=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment">#lr = 0.1</span></span><br><span class="line">    <span class="comment">#sh_lr = theano.shared(lasagne.utils.floatX(lr))</span></span><br><span class="line">    <span class="comment">#updates = lasagne.updates.momentum(</span></span><br><span class="line">    <span class="comment">#        loss, params, learning_rate=sh_lr, momentum=0.9)</span></span><br><span class="line">    updates = lasagne.updates.adagrad(loss, params)</span><br><span class="line">    <span class="comment"># Create a loss expression for validation/testing</span></span><br><span class="line">    test_prediction = lasagne.layers.get_output(network)</span><br><span class="line">    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,</span><br><span class="line">                                                            target_var)</span><br><span class="line">    test_loss = test_loss.mean()</span><br><span class="line">    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=<span class="number">1</span>), target_var),</span><br><span class="line">                      dtype=theano.config.floatX)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compile a function performing a training step on a mini-batch (by giving</span></span><br><span class="line">    <span class="comment"># the updates dictionary) and returning the corresponding training loss:</span></span><br><span class="line">    train_fn = theano.function([input_var, target_var], loss, updates=updates)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compile a second function computing the validation loss and accuracy:</span></span><br><span class="line">    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Finally, launch the training loop.</span></span><br><span class="line">    print(<span class="string">"Starting training..."</span>)</span><br><span class="line">    <span class="comment"># We iterate over epochs:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="comment"># In each epoch, we do a full pass over the training data:</span></span><br><span class="line">        train_err = <span class="number">0</span></span><br><span class="line">        train_batches = <span class="number">0</span></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterate_minibatches(X_train, Y_train, <span class="number">512</span>, shuffle=<span class="keyword">True</span>, augment=<span class="keyword">True</span>):</span><br><span class="line">            inputs, targets = batch</span><br><span class="line">            train_err += train_fn(inputs, targets)</span><br><span class="line">            train_batches += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        train_acc = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterate_minibatches(X_train, Y_train, <span class="number">500</span>, shuffle=<span class="keyword">False</span>):</span><br><span class="line">            inputs, targets = batch</span><br><span class="line">            _, acc = val_fn(inputs, targets)</span><br><span class="line">            train_acc += acc</span><br><span class="line">        <span class="comment"># And a full pass over the validation data:</span></span><br><span class="line">        val_err = <span class="number">0</span></span><br><span class="line">        val_acc = <span class="number">0</span></span><br><span class="line">        val_batches = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterate_minibatches(X_test, Y_test, <span class="number">500</span>, shuffle=<span class="keyword">False</span>):</span><br><span class="line">            inputs, targets = batch</span><br><span class="line">            err, acc = val_fn(inputs, targets)</span><br><span class="line">            val_err += err</span><br><span class="line">            val_acc += acc</span><br><span class="line">            val_batches += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Then we print the results for this epoch:</span></span><br><span class="line">        print(<span class="string">"Epoch &#123;&#125; of &#123;&#125; took &#123;:.3f&#125;s"</span>.format(</span><br><span class="line">            epoch + <span class="number">1</span>, num_epochs, time.time() - start_time))</span><br><span class="line">        print(<span class="string">"  training loss:\t\t&#123;:.6f&#125;"</span>.format(train_err / train_batches))</span><br><span class="line">        print(<span class="string">"  training accuracy:\t\t&#123;:.2f&#125; %"</span>.format(train_acc / train_batches * <span class="number">100</span>))</span><br><span class="line">        print(<span class="string">"  validation loss:\t\t&#123;:.6f&#125;"</span>.format(val_err / val_batches))</span><br><span class="line">        print(<span class="string">"  validation accuracy:\t\t&#123;:.2f&#125; %"</span>.format(</span><br><span class="line">            val_acc / val_batches * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adjust learning rate as in paper</span></span><br><span class="line">        <span class="comment"># 32k and 48k iterations should be roughly equivalent to 41 and 61 epochs</span></span><br><span class="line">        <span class="comment">#If (epoch+1) == 41 or (epoch+1) == 61:</span></span><br><span class="line">            <span class="comment">#new_lr = sh_lr.get_value() * 0.1</span></span><br><span class="line">            <span class="comment">#print("New LR:"+str(new_lr))</span></span><br><span class="line">            <span class="comment">#sh_lr.set_value(lasagne.utils.floatX(new_lr))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># After training, we compute and print the test error:</span></span><br><span class="line">    test_err = <span class="number">0</span></span><br><span class="line">    test_acc = <span class="number">0</span></span><br><span class="line">    test_batches = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterate_minibatches(X_test, Y_test, <span class="number">500</span>, shuffle=<span class="keyword">False</span>):</span><br><span class="line">        inputs, targets = batch</span><br><span class="line">        err, acc = val_fn(inputs, targets)</span><br><span class="line">        test_err += err</span><br><span class="line">        test_acc += acc</span><br><span class="line">        test_batches += <span class="number">1</span></span><br><span class="line">    print(<span class="string">"Final results:"</span>)</span><br><span class="line">    print(<span class="string">"  test loss:\t\t\t&#123;:.6f&#125;"</span>.format(test_err / test_batches))</span><br><span class="line">    print(<span class="string">"  test accuracy:\t\t&#123;:.2f&#125; %"</span>.format(</span><br><span class="line">        test_acc / test_batches * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dump the network weights to a file :</span></span><br><span class="line">    np.savez(<span class="string">'spoof_deep_residual_model.npz'</span>, *lasagne.layers.get_all_param_values(network))</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># And load them again later on like this:</span></span><br><span class="line">    <span class="comment"># with np.load('cifar10_deep_residual_model.npz') as f:</span></span><br><span class="line">    <span class="comment">#     param_values = [f['arr_%d' % i] for i in range(len(f.files))]</span></span><br><span class="line">    <span class="comment"># lasagne.layers.set_all_param_values(network, param_values)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">if</span> (<span class="string">'--help'</span> <span class="keyword">in</span> sys.argv) <span class="keyword">or</span> (<span class="string">'-h'</span> <span class="keyword">in</span> sys.argv):</span><br><span class="line">        print(<span class="string">"Trains a Deep Residual Learning network on cifar-10 using Lasagne."</span>)</span><br><span class="line">        print(<span class="string">"Network architecture and training parameters are as in section 4.2 in 'Deep Residual Learning for Image Recognition'."</span>)</span><br><span class="line">        print(<span class="string">"Usage: %s [N [EPOCHS]]"</span> % sys.argv[<span class="number">0</span>])</span><br><span class="line">        print()</span><br><span class="line">        print(<span class="string">"N: Number of stacked residual building blocks per feature map (default: 5)"</span>)</span><br><span class="line">        print(<span class="string">"EPOCHS: number of training epochs to perform (default: 82)"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        kwargs = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> len(sys.argv) &gt; <span class="number">1</span>:</span><br><span class="line">            kwargs[<span class="string">'n'</span>] = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> len(sys.argv) &gt; <span class="number">2</span>:</span><br><span class="line">            kwargs[<span class="string">'num_epochs'</span>] = int(sys.argv[<span class="number">3</span>])</span><br><span class="line">        main(**kwargs)</span><br></pre></td></tr></table></figure>
<p>Update: Found the reason. For this task batch normalization shouldn’t be used otherwise the accuracy is really low(&lt;40%). In my opinion the Batch Normalization may not be a good idea for speech processing tasks. The original paper published by Google also mentioned that it largely improve the performance using sigmoid activation function instead of rectifier function.</p>
</span>
      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/12/31/Review-of-2015/" rel="next" title="Review of 2015 ">
                <i class="fa fa-chevron-left"></i> Review of 2015 
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/03/20/Attention-model/" rel="prev" title="Attention Model">
                Attention Model <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


        </div>

        


        
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      </div>
    
  </div>


      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="https://avatars0.githubusercontent.com/u/939088?v=3&s=460" alt="Nanxin Chen" itemprop="image"/>
          <p class="site-author-name" itemprop="name"><a href="mailto:bobchennan@gmail.com">Nanxin Chen</a></p>
        </div>
        <p class="site-description motion-element" itemprop="description">Write life of CNX</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">22</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">6</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">18</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/bobchennan" target="_blank">
                  
                    <i class="fa fa-github"></i> GitHub
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/cnx" target="_blank">
                  
                    <i class="fa fa-twitter"></i> Twitter
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.facebook.com/bobchennan" target="_blank">
                  
                    <i class="fa fa-facebook"></i> Facebook
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/nanxin-chen-957a6932" target="_blank">
                  
                    <i class="fa fa-linkedin"></i> Linkedin
                  
                </a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2014 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nanxin Chen</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  

  
    
    

  

    <script type="text/javascript">
      var disqus_shortname = 'bobchennan';
      var disqus_identifier = '2016/01/23/Network-Training-for-Speaker-Verification-Spoofing-Detection/';
      var disqus_title = 'Network Training for Speaker Verification/ Spoofing Detection';
      var disqus_url = 'http://www.myemacs.com/2016/01/23/Network-Training-for-Speaker-Verification-Spoofing-Detection/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  


  

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
<script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

<script type="text/javascript" src="/js/motion.js?v=0.4.5.2" id="motion.global"></script>


  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 1 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    motionMiddleWares.sidebar = function () {
      var $tocContent = $('.post-toc-content');
      if (CONFIG.sidebar === 'post') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          displaySidebar();
        }
      }
    };
  });
</script>



  <script type="text/javascript" src="/js/bootstrap.js"></script>

  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>

  
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  
  

</body>
</html>
